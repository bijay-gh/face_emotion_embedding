{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed72272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from model import SwinWithSE, SEBlock, train_one_epoch, evaluate\n",
    "\n",
    "# Tell PyTorch itâ€™s safe to unpickle SwinWithSE\n",
    "# torch.serialization.add_safe_globals({'SwinWithSE': SwinWithSE})\n",
    "\n",
    "model =SwinWithSE(7)\n",
    "criterion =torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f35d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emotion_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edef9d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinWithSE(\n",
      "  (backbone): SwinTransformer(\n",
      "    (features): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (1): Permute()\n",
      "        (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): PatchMerging(\n",
      "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): PatchMerging(\n",
      "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): PatchMerging(\n",
      "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): ShiftedWindowAttention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (4): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (permute): Permute()\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (se): SEBlock(\n",
      "    (global_avg): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=48, bias=False)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=48, out_features=768, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('swin_with_se_fer2013_full.pth', map_location=torch.device(device), weights_only=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ecf0a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.2 c:\\Users\\Bijaya\\anaconda3\\envs\\emotion_detection\\lib\\site-packages\\numpy\\__init__.py\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "print(numpy.__version__, numpy.__file__); print(hasattr(numpy, 'ndarray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b25700ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Preprocessing transform (match your training pipeline)\n",
    "preprocess =  transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Emotion labels\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4faa1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96ae2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "emoji_path = './emoji/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b3402f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    # Convert BGR (OpenCV) to RGB for MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect faces using MediaPipe\n",
    "    results = face_detection.process(frame_rgb)\n",
    "    \n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "            ih, iw, _ = frame.shape\n",
    "            x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "            \n",
    "            # Ensure coordinates are within frame bounds\n",
    "            x, y = max(0, x), max(0, y)\n",
    "            w, h = min(w, iw-x), min(h, ih-y)\n",
    "            \n",
    "            # Extract face region\n",
    "            face_roi = frame_rgb[y:y+h, x:x+w]\n",
    "            if face_roi.size == 0:  # Skip if ROI is empty\n",
    "                continue\n",
    "            \n",
    "            # Preprocess face for model\n",
    "            face_pil = Image.fromarray(face_roi)\n",
    "            face_tensor = preprocess(face_pil).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Predict emotion\n",
    "            with torch.no_grad():\n",
    "                output = model(face_tensor)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                emotion = emotion_labels[predicted.item()]\n",
    "                emoji = './emoji/' + emotion + '.png'\n",
    "                print(emotion, emoji)\n",
    "            \n",
    "            # Draw rectangle and label on frame\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            \n",
    "    # Encode frame as JPEG\n",
    "    ret, buffer = cv2.imencode('.jpg', frame)\n",
    "    frame_bytes = buffer.tobytes()\n",
    "    return frame_bytes\n",
    "def generate_frames_continuously():\n",
    "    cap = cv2.VideoCapture(0)  # Open default webcam\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        print(\"Webcam opened successfully. Press 'q' to quit.\")\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Error: Could not read frame. Restarting capture...\")\n",
    "                time.sleep(1)  # Brief pause before retry\n",
    "                cap = cv2.VideoCapture(0)  # Attempt to reopen\n",
    "                if not cap.isOpened():\n",
    "                    print(\"Error: Failed to reopen webcam. Exiting.\")\n",
    "                    break\n",
    "                continue\n",
    "            \n",
    "            # Process the frame (placeholder; replace with your logic)\n",
    "            frame_bytes = process_frame(frame)\n",
    "            if frame_bytes is None:\n",
    "                print(\"Warning: process_frame returned None. Skipping frame.\")\n",
    "                continue\n",
    "            \n",
    "            # Convert frame_bytes back to image for display (assuming JPEG)\n",
    "            frame_img = cv2.imdecode(np.frombuffer(frame_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
    "            if frame_img is not None:\n",
    "                # Display the frame\n",
    "                cv2.imshow('Webcam Feed', frame_img)\n",
    "                # Break the loop if 'q' is pressed\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    print(\"Quitting...\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Warning: Failed to decode frame.\")\n",
    "            \n",
    "            time.sleep(1.0 / 30)  # Approx 30 FPS for smooth display\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped by user (Ctrl+C). Closing webcam...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()  # Close all OpenCV windows\n",
    "        print(\"Webcam and windows released.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "185e4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "emoji_path = './emoji/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eee98f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam opened successfully. Press 'q' to quit.\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "neutral ./emoji/neutral.png\n",
      "sad ./emoji/sad.png\n",
      "neutral ./emoji/neutral.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "neutral ./emoji/neutral.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "sad ./emoji/sad.png\n",
      "Quitting...\n",
      "Webcam and windows released.\n"
     ]
    }
   ],
   "source": [
    "generate_frames_continuously()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa54b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
